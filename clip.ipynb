{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "7d21335d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import random\n",
        "from PIL import Image\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import CLIPModel,CLIPProcessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "3191bded",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_split_file(file_path):\n",
        "    with open(file_path,'r') as f:\n",
        "        lines= f.readlines()\n",
        "    samples = []\n",
        "    for line in lines:\n",
        "        path, label= line.strip().split()\n",
        "        samples.append((path, int(label)))\n",
        "    return samples\n",
        "\n",
        "def load_class_names(file_path):\n",
        "    id_to_name = {}\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            idx, name = line.split()[0],re.sub(r'\\d+', '', line).strip()\n",
        "\n",
        "            id_to_name[int(idx)] = name\n",
        "    return id_to_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "d0e507b7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "ef972053",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_root = '../ip102_v1.1/'\n",
        "train_txt = os.path.join(dataset_root, \"train.txt\")\n",
        "val_txt = os.path.join(dataset_root, \"val.txt\")\n",
        "test_txt = os.path.join(dataset_root, \"test.txt\")\n",
        "images_root=os.path.join(dataset_root, \"images\")\n",
        "\n",
        "\n",
        "train_data = load_split_file(train_txt)\n",
        "test_data = load_split_file(test_txt)\n",
        "val_data = load_split_file(val_txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "38bf3963",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CLIPModel(\n",
              "  (text_model): CLIPTextTransformer(\n",
              "    (embeddings): CLIPTextEmbeddings(\n",
              "      (token_embedding): Embedding(49408, 512)\n",
              "      (position_embedding): Embedding(77, 512)\n",
              "    )\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x CLIPEncoderLayer(\n",
              "          (self_attn): CLIPSdpaAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): QuickGELUActivation()\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (vision_model): CLIPVisionTransformer(\n",
              "    (embeddings): CLIPVisionEmbeddings(\n",
              "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "      (position_embedding): Embedding(50, 768)\n",
              "    )\n",
              "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x CLIPEncoderLayer(\n",
              "          (self_attn): CLIPSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): QuickGELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
              "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_name = \"openai/clip-vit-base-patch32\"\n",
        "model = CLIPModel.from_pretrained(model_name).to(device)\n",
        "processor = CLIPProcessor.from_pretrained(model_name)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8533aa97",
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_model_few_shot(path, k_shots=0):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    id_to_name = load_class_names(path)\n",
        "    num_classes = len(id_to_name)\n",
        "\n",
        "    if k_shots == 0:\n",
        "        text_prompts = [f\"a photo of a {name}\" for name in list(id_to_name.values())]\n",
        "        text_inputs = processor(text=text_prompts, return_tensors=\"pt\", padding=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            try:\n",
        "                text_features = model.get_text_features(**text_inputs)\n",
        "            except:\n",
        "                text_inputs = processor(text=text_prompts,return_tensors=\"pt\",padding=\"max_length\", \n",
        "                            truncation=True,max_length=processor.tokenizer.model_max_length ).to(device)\n",
        "        text_features /= text_features.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "    else:\n",
        "        print(f\"Preparing {k_shots}-shot prototypes...\")\n",
        "        class_features_list = [[] for _ in range(num_classes)]\n",
        "        shot_counts = {i: 0 for i in range(num_classes)}\n",
        "        random.shuffle(train_data)\n",
        "\n",
        "        for image_path, true_label in train_data:\n",
        "            if shot_counts[true_label] < k_shots:\n",
        "                full_path = os.path.join(images_root, image_path)\n",
        "                image = Image.open(full_path).convert(\"RGB\")\n",
        "                inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    image_features = model.get_image_features(**inputs)\n",
        "                    image_features /= image_features.norm(p=2, dim=-1, keepdim=True)\n",
        "                    class_features_list[true_label].append(image_features)\n",
        "                    shot_counts[true_label] += 1\n",
        "            \n",
        "            if all(count == k_shots for count in shot_counts.values()):\n",
        "                break\n",
        "        \n",
        "        class_features = torch.zeros(num_classes, model.config.projection_dim).to(device)\n",
        "        for i in range(num_classes):\n",
        "            if class_features_list[i]:\n",
        "                class_features[i] = torch.stack(class_features_list[i]).mean(dim=0).squeeze(0)\n",
        "            else:\n",
        "                print(f\"Warning: Not enough shots collected\")\n",
        "        class_features /= class_features.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "    print(f\"Starting evaluation with {k_shots}-shot learning...\")\n",
        "    \n",
        "    for image_path, true_label in tqdm(val_data): \n",
        "        full_path = os.path.join(images_root, image_path)\n",
        "        image = Image.open(full_path).convert(\"RGB\")\n",
        "        \n",
        "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            image_features = model.get_image_features(**inputs)\n",
        "            image_features /= image_features.norm(p=2, dim=-1, keepdim=True)\n",
        "            \n",
        "            similarity = (image_features @ class_features.T).squeeze(0)  \n",
        "            pred_label = similarity.argmax().item()\n",
        "            \n",
        "            if pred_label == true_label:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "    accuracy = correct / total * 100\n",
        "    print(f\"Test Accuracy ({k_shots}-shot): {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e692ad73",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 5-shot Evaluation ---\n",
            "Starting evaluation with 0-shot learning...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21a64aa4f5104e26986c2e4494b4ac52",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/7508 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy (0-shot): 22.11%\n"
          ]
        }
      ],
      "source": [
        "test_model_few_shot(os.path.join(dataset_root, \"../large-multi-modal/caption generation/gemini-small.txt\"), k_shots=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "927e5747",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing 10-shot prototypes...\n",
            "Starting evaluation with 10-shot learning...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f496e46a1a3b498d802413bde60933ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/7508 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy (10-shot): 27.85%\n"
          ]
        }
      ],
      "source": [
        "test_model_few_shot(os.path.join(dataset_root, \"../large-multi-modal/caption generation/gemini-small.txt\"), k_shots=10)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
