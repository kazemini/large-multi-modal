{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6eff3cdf",
      "metadata": {},
      "source": [
        "## Import Essential Library And Get Data Ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d21335d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import random\n",
        "from PIL import Image\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import CLIPModel,CLIPProcessor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "3191bded",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_split_file(file_path):\n",
        "    with open(file_path,'r') as f:\n",
        "        lines= f.readlines()\n",
        "    samples = []\n",
        "    for line in lines:\n",
        "        path, label= line.strip().split()\n",
        "        samples.append((path, int(label)))\n",
        "    return samples\n",
        "\n",
        "def load_prompt(file_path):\n",
        "    id_to_name = {}\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            idx, name = line.split()[0],re.sub(r'\\d+', '', line).strip()\n",
        "\n",
        "            id_to_name[int(idx)] = name\n",
        "    return id_to_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d0e507b7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "ef972053",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_root = '../ip102_v1.1/'\n",
        "train_txt = os.path.join(dataset_root, \"train.txt\")\n",
        "val_txt = os.path.join(dataset_root, \"val.txt\")\n",
        "test_txt = os.path.join(dataset_root, \"test.txt\")\n",
        "images_root=os.path.join(dataset_root, \"images\")\n",
        "\n",
        "\n",
        "train_data = load_split_file(train_txt)\n",
        "test_data = load_split_file(test_txt)\n",
        "val_data = load_split_file(val_txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "ae97df4b",
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, data_list, root_dir, prompts, transform=None):\n",
        "        self.data_list = data_list\n",
        "        self.root_dir = root_dir    \n",
        "        self.prompts = prompts\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.data_list[idx]\n",
        "        image_path = os.path.join(self.root_dir, path)\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        prompt = self.prompts[label]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image,prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "3caa720b",
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "prompts= list(load_prompt('../large-multi-modal/caption generation/gemini-short.txt').values())\n",
        "\n",
        "train_dataset = CustomImageDataset(train_data, images_root,prompts, transform=transform,)\n",
        "val_dataset = CustomImageDataset(val_data, images_root,prompts, transform=transform)\n",
        "test_dataset = CustomImageDataset(test_data, images_root,prompts, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab02d433",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = \"openai/clip-vit-base-patch16\"\n",
        "model = CLIPModel.from_pretrained(model_name).to(device)\n",
        "processor = CLIPProcessor.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "800913ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# فقط LoRA روی image encoder (ViT)\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # فقط روی attention\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.FEATURE_EXTRACTION  # چون فقط encoding تصویر/متن می‌خوایم\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d5793fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    images, texts = zip(*batch)\n",
        "    inputs = processor(text=list(texts), images=list(images), return_tensors=\"pt\", padding=True)\n",
        "    return inputs\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4064231d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "for batch in train_loader:\n",
        "    inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    logits_per_image = outputs.logits_per_image  # (B, B)\n",
        "    labels = torch.arange(len(logits_per_image)).to(device)\n",
        "    \n",
        "    loss = criterion(logits_per_image, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
